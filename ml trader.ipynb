{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import robin_stocks as r\n",
    "import inspect\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trade options using RL - https://towardsdatascience.com/deep-reinforcement-learning-for-automated-stock-trading-f1dad0126a02\n",
    "\n",
    "Also use robintrack for popularity data\n",
    "## State space definition\n",
    "Select 10 options around the atm price  \n",
    "1. cash - money held as cash - 1  \n",
    "2. price - bid prices of options we can trade - n    \n",
    "    3. vegas - list of vega for each option - n  \n",
    "    4. gammma - list of gammas for each option - n   \n",
    "    5. deltas - list of deltas for each option - n  \n",
    "    6. thetas - list of thetas for each option - n  \n",
    "    7. ivs - list of ivs for each option - n\n",
    "3.  how many days to earning - 1  \n",
    "4. for each options how many days from earnings is the option expiration date - n  \n",
    "8. spy - spy price - 1 \n",
    "9. stock - price of share - 1   \n",
    "10. position - for each option are we -1,0 or 1 - n  \n",
    "11. cost - for each option what was our purchase price - n \n",
    "12. historical price of option - array of price of options for last 7 days - 7n  \n",
    "13. historical price of stock - array of price of stock for last 7 days - 7n  \n",
    "\n",
    "## Action space definition\n",
    "a - buy/sell/hold - for each option either -1,0,1 - n  \n",
    "  \n",
    "## Reward function\n",
    "r(s,a,s') - cash + price * position\n",
    "\n",
    "## Policy\n",
    "p(s) - probability distribution of actions to take at s\n",
    "\n",
    "## Q-value\n",
    "Q(a,s) - expected value of taking action a at state s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import robin_stocks as r\n",
    "login = r.login('anshul.tibrewal2203@gmail.com', \"Icangeta45!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '7dd906e5-7d4b-4161-a3fe-2c3b62038482',\n",
       " 'symbol': 'AAPL',\n",
       " 'can_open_position': True,\n",
       " 'cash_component': None,\n",
       " 'expiration_dates': ['2020-10-02',\n",
       "  '2020-10-09',\n",
       "  '2020-10-16',\n",
       "  '2020-10-23',\n",
       "  '2020-10-30',\n",
       "  '2020-11-06',\n",
       "  '2020-11-20',\n",
       "  '2020-12-18',\n",
       "  '2021-01-15',\n",
       "  '2021-03-19',\n",
       "  '2021-04-16',\n",
       "  '2021-06-18',\n",
       "  '2021-09-17',\n",
       "  '2022-01-21',\n",
       "  '2022-06-17',\n",
       "  '2022-09-16',\n",
       "  '2023-01-20'],\n",
       " 'trade_value_multiplier': '100.0000',\n",
       " 'underlying_instruments': [{'id': '3b1b2528-8887-4410-bce4-b5128eac4a86',\n",
       "   'instrument': 'https://api.robinhood.com/instruments/450dfc6d-5510-4d40-abfb-f633b7d9be3e/',\n",
       "   'quantity': 100}],\n",
       " 'min_ticks': {'above_tick': '0.05',\n",
       "  'below_tick': '0.01',\n",
       "  'cutoff_price': '3.00'}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!!! fill out the specific option information\n",
    "symbol = 'AAPL'\n",
    "symbol_name = r.get_name_by_symbol(symbol)\n",
    "expirationDate = '2020-10-09' # format is YYYY-MM-DD.\n",
    "strike = 300\n",
    "optionType = 'call' # available options are 'call' or 'put' or None.\n",
    "interval = '10minute' # available options are '5minute', '10minute', 'hour', 'day', and 'week'.\n",
    "span = 'week' # available options are 'day', 'week', 'year', and '5year'.\n",
    "bounds = 'regular' # available options are 'regular', 'trading', and 'extended'.\n",
    "info = None\n",
    "#!!!\n",
    "r.get_chains(symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r.get_open_option_positions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the option ID failed. Perhaps the expiration date is wrong format, or the strike price is wrong.\n",
      "404 Client Error: Not Found for url: https://api.robinhood.com/marketdata/options/historicals/None/?span=week&interval=10minute&bounds=regular\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-16-7021a040b637>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mopenPrices\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m \u001B[0;32mfor\u001B[0m \u001B[0mdata_point\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mhistoricalData\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      8\u001B[0m     \u001B[0mdates\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata_point\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'begins_at'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m     \u001B[0mclosingPrices\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata_point\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'close_price'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "historicalData = r.get_option_historicals(symbol, expirationDate, strike, optionType, interval, span, bounds)\n",
    "\n",
    "dates = []\n",
    "closingPrices = []\n",
    "openPrices = []\n",
    "\n",
    "for data_point in historicalData:\n",
    "    dates.append(data_point['begins_at'])\n",
    "    closingPrices.append(data_point['close_price'])\n",
    "    openPrices.append(data_point['open_price'])\n",
    "\n",
    "# change the dates into a format that matplotlib can recognize.\n",
    "x = [dt.datetime.strptime(d,'%Y-%m-%dT%H:%M:%SZ') for d in dates]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all stock data we need for state space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_state(ticker=\"AAPL\"):\n",
    "    stock_prices = r.get_stock_historicals(ticker, interval='day', span='3month', bounds='regular')\n",
    "    earnings = r.stocks.get_earnings(ticker)\n",
    "    return {\"P\" : stock_prices, \"e\" : earnings}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'robin_stocks' has no attribute 'get_stock_historicals'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-20-9c1e630b7b8a>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mget_stock_state\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-19-a8144a2bf675>\u001B[0m in \u001B[0;36mget_stock_state\u001B[0;34m(ticker)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mget_stock_state\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mticker\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"AAPL\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m     \u001B[0mstock_prices\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_stock_historicals\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mticker\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minterval\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'day'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mspan\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'3month'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbounds\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'regular'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m     \u001B[0mearnings\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstocks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_earnings\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mticker\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0;34m\"P\"\u001B[0m \u001B[0;34m:\u001B[0m \u001B[0mstock_prices\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"e\"\u001B[0m \u001B[0;34m:\u001B[0m \u001B[0mearnings\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAttributeError\u001B[0m: module 'robin_stocks' has no attribute 'get_stock_historicals'"
     ]
    }
   ],
   "source": [
    "get_stock_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Additional pages.\n",
      "Loading page 2 ...\n",
      "Loading page 3 ...\n",
      "Loading page 4 ...\n",
      "Loading page 5 ...\n",
      "Loading page 6 ...\n",
      "Loading page 7 ...\n",
      "Loading page 8 ...\n",
      "Loading page 9 ...\n",
      "Loading page 10 ...\n",
      "Loading page 11 ...\n",
      "Loading page 12 ...\n",
      "Loading page 13 ...\n",
      "Loading page 14 ...\n",
      "Loading page 15 ...\n",
      "Loading page 16 ...\n",
      "Loading page 17 ...\n",
      "Loading page 18 ...\n",
      "Loading page 19 ...\n",
      "Loading page 20 ...\n",
      "Loading page 21 ...\n",
      "Loading page 22 ...\n",
      "Loading page 23 ...\n",
      "Loading page 24 ...\n",
      "Loading page 25 ...\n",
      "Loading page 26 ...\n",
      "Loading page 27 ...\n",
      "Getting the option ID failed. Perhaps the expiration date is wrong format, or the strike price is wrong.\n",
      "404 Client Error: Not Found for url: https://api.robinhood.com/marketdata/options/historicals/None/?span=week&interval=day&bounds=regular\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-50-ebea6a6fac2b>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0mopenPrices\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 17\u001B[0;31m \u001B[0;32mfor\u001B[0m \u001B[0mitem\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mhistoricalData\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     18\u001B[0m \u001B[0;31m#     print(item)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m     \u001B[0mdates\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdatetime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstrptime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mitem\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'begins_at'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'%Y-%m-%dT%H:%M:%SZ'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "!!! fill out the specific option information\n",
    "symbol = 'AAPL'\n",
    "expirationDate = '2020-09-25'\n",
    "strike = 200\n",
    "optionType = 'call'\n",
    "span = 'day' #available options are day,week,year, and 5year\n",
    "#!!!\n",
    "days = 20\n",
    "chain = r.options.get_chains(symbol)\n",
    "tradeable = r.options.find_tradable_options(symbol = symbol, expirationDate = chain[\"expiration_dates\"][0])\n",
    "historicalData = r.get_option_historicals(symbol,expirationDate,strike,optionType,span)\n",
    "\n",
    "dates = []\n",
    "closingPrices = []\n",
    "openPrices = []\n",
    "\n",
    "for item in historicalData:\n",
    "#     print(item)\n",
    "    dates.append(dt.datetime.strptime(item['begins_at'],'%Y-%m-%dT%H:%M:%SZ'))\n",
    "    closingPrices.append(float(item['close_price']))\n",
    "    openPrices.append(float(item['open_price']))\n",
    "\n",
    "# print(openPrices)\n",
    "#!!! I made it so it only got the last 30 days but you could delete these lines to get full year.\n",
    "dates = dates[-days:]\n",
    "closingPrices = closingPrices[-days:]\n",
    "openPrices = openPrices[-days:]\n",
    "#\n",
    "start_date = dates[0]\n",
    "\n",
    "\n",
    "# change the dates into a format that matplotlib can recognize.\n",
    "# x = [dt.datetime.strptime(d,'%Y-%m-%dT%H:%M:%SZ') for d in dates]\n",
    "x = []\n",
    "for d in dates:\n",
    "    seconds = (d - start_date).total_seconds()/86400\n",
    "    x.append(seconds)\n",
    "    \n",
    "\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "# plots the data.\n",
    "plt.plot(x, closingPrices, 'ro')\n",
    "# plt.plot(x, openPrices, 'bo')\n",
    "plt.title(\"Option price for \"+r.get_name_by_symbol(symbol)+\" over time\")\n",
    "plt.xlabel(\"Dates\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'begins_at': '2020-09-14T13:30:00Z', 'open_price': '20.430000', 'close_price': '20.430000', 'high_price': '20.430000', 'low_price': '20.430000', 'volume': 0, 'session': 'reg', 'interpolated': False, 'symbol': 'ENPH'}\n"
     ]
    }
   ],
   "source": [
    "print(historicalData[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Soft Actor Critic in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxillary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedActions():\n",
    "    def _action(self, action):\n",
    "        low  = -1\n",
    "        high = 1\n",
    "        \n",
    "        action = low + (action + 1.0) * 0.5 * (high - low)\n",
    "        action = np.clip(action, low, high)\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def _reverse_action(self, action):\n",
    "        low  = -1\n",
    "        high = 1\n",
    "        \n",
    "        action = 2 * (action - low) / (high - low) - 1\n",
    "        action = np.clip(action, low, high)\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, init_w=3e-3):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear3 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "        self.linear3.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "class SoftQNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size, init_w=3e-3):\n",
    "        super(SoftQNetwork, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs + num_actions, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "        self.linear3.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], 1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "        \n",
    "        \n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size, init_w=3e-3, log_std_min=-20, log_std_max=2):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        self.mean_linear = nn.Linear(hidden_size, num_actions)\n",
    "        self.mean_linear.weight.data.uniform_(-init_w, init_w)\n",
    "        self.mean_linear.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "        self.log_std_linear = nn.Linear(hidden_size, num_actions)\n",
    "        self.log_std_linear.weight.data.uniform_(-init_w, init_w)\n",
    "        self.log_std_linear.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        \n",
    "        mean    = self.mean_linear(x)\n",
    "        log_std = self.log_std_linear(x)\n",
    "        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)\n",
    "        \n",
    "        return mean, log_std\n",
    "    \n",
    "    def evaluate(self, state, epsilon=1e-6):\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        \n",
    "        normal = Normal(0, 1)\n",
    "        z      = normal.sample()\n",
    "        action = torch.tanh(mean+ std*z.to(device))\n",
    "        log_prob = Normal(mean, std).log_prob(mean+ std*z.to(device)) - torch.log(1 - action.pow(2) + epsilon)\n",
    "        return action, log_prob, z, mean, log_std\n",
    "        \n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        \n",
    "        normal = Normal(0, 1)\n",
    "        z      = normal.sample().to(device)\n",
    "        action = torch.tanh(mean + std*z)\n",
    "        \n",
    "        action  = action.cpu()#.detach().cpu().numpy()\n",
    "        return action[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(batch_size,gamma=0.99,soft_tau=1e-2,):\n",
    "    \n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "\n",
    "    state      = torch.FloatTensor(state).to(device)\n",
    "    next_state = torch.FloatTensor(next_state).to(device)\n",
    "    action     = torch.FloatTensor(action).to(device)\n",
    "    reward     = torch.FloatTensor(reward).unsqueeze(1).to(device)\n",
    "    done       = torch.FloatTensor(np.float32(done)).unsqueeze(1).to(device)\n",
    "\n",
    "    predicted_q_value1 = soft_q_net1(state, action)\n",
    "    predicted_q_value2 = soft_q_net2(state, action)\n",
    "    predicted_value    = value_net(state)\n",
    "    new_action, log_prob, epsilon, mean, log_std = policy_net.evaluate(state)\n",
    "\n",
    "    \n",
    "    \n",
    "# Training Q Function\n",
    "    target_value = target_value_net(next_state)\n",
    "    target_q_value = reward + (1 - done) * gamma * target_value\n",
    "    q_value_loss1 = soft_q_criterion1(predicted_q_value1, target_q_value.detach())\n",
    "    q_value_loss2 = soft_q_criterion2(predicted_q_value2, target_q_value.detach())\n",
    "\n",
    "\n",
    "    soft_q_optimizer1.zero_grad()\n",
    "    q_value_loss1.backward()\n",
    "    soft_q_optimizer1.step()\n",
    "    soft_q_optimizer2.zero_grad()\n",
    "    q_value_loss2.backward()\n",
    "    soft_q_optimizer2.step()    \n",
    "# Training Value Function\n",
    "    predicted_new_q_value = torch.min(soft_q_net1(state, new_action),soft_q_net2(state, new_action))\n",
    "    target_value_func = predicted_new_q_value - log_prob\n",
    "    value_loss = value_criterion(predicted_value, target_value_func.detach())\n",
    "\n",
    "    \n",
    "    value_optimizer.zero_grad()\n",
    "    value_loss.backward()\n",
    "    value_optimizer.step()\n",
    "# Training Policy Function\n",
    "    policy_loss = (log_prob - predicted_new_q_value).mean()\n",
    "\n",
    "    policy_optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    policy_optimizer.step()\n",
    "    \n",
    "    \n",
    "    for target_param, param in zip(target_value_net.parameters(), value_net.parameters()):\n",
    "        target_param.data.copy_(\n",
    "            target_param.data * (1.0 - soft_tau) + param.data * soft_tau\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-12-22651aa71a12>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0menv\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mNormalizedActions\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgym\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmake\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Pendulum-v0\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0maction_dim\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0maction_space\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mstate_dim\u001B[0m  \u001B[0;34m=\u001B[0m \u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mobservation_space\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mhidden_dim\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m256\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'gym' is not defined"
     ]
    }
   ],
   "source": [
    "env = NormalizedActions()\n",
    "\n",
    "action_dim = env.action_space.shape[0]\n",
    "state_dim  = env.observation_space.shape[0]\n",
    "hidden_dim = 256\n",
    "\n",
    "value_net        = ValueNetwork(state_dim, hidden_dim).to(device)\n",
    "target_value_net = ValueNetwork(state_dim, hidden_dim).to(device)\n",
    "\n",
    "soft_q_net1 = SoftQNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "soft_q_net2 = SoftQNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "policy_net = PolicyNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "\n",
    "for target_param, param in zip(target_value_net.parameters(), value_net.parameters()):\n",
    "    target_param.data.copy_(param.data)\n",
    "    \n",
    "\n",
    "value_criterion  = nn.MSELoss()\n",
    "soft_q_criterion1 = nn.MSELoss()\n",
    "soft_q_criterion2 = nn.MSELoss()\n",
    "\n",
    "value_lr  = 3e-4\n",
    "soft_q_lr = 3e-4\n",
    "policy_lr = 3e-4\n",
    "\n",
    "value_optimizer  = optim.Adam(value_net.parameters(), lr=value_lr)\n",
    "soft_q_optimizer1 = optim.Adam(soft_q_net1.parameters(), lr=soft_q_lr)\n",
    "soft_q_optimizer2 = optim.Adam(soft_q_net2.parameters(), lr=soft_q_lr)\n",
    "policy_optimizer = optim.Adam(policy_net.parameters(), lr=policy_lr)\n",
    "\n",
    "\n",
    "replay_buffer_size = 1000000\n",
    "replay_buffer = ReplayBuffer(replay_buffer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'frame_idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-14-942b399e34f6>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mwhile\u001B[0m \u001B[0mframe_idx\u001B[0m \u001B[0;34m<\u001B[0m \u001B[0mmax_frames\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m     \u001B[0mstate\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m     \u001B[0mepisode_reward\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mstep\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmax_steps\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'frame_idx' is not defined"
     ]
    }
   ],
   "source": [
    "while frame_idx < max_frames:\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        if frame_idx >1000:\n",
    "            action = policy_net.get_action(state).detach()\n",
    "            next_state, reward, done, _ = env.step(action.numpy())\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        \n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "        \n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        frame_idx += 1\n",
    "        \n",
    "        if len(replay_buffer) > batch_size:\n",
    "            update(batch_size)\n",
    "        \n",
    "        if frame_idx % 1000 == 0:\n",
    "            plot(frame_idx, rewards)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    rewards.append(episode_reward)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}